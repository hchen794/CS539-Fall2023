{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yJfYGSeBGjmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SSBjb8ZGXdv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# P1) Analyze the dataset\n",
        "corpus = [line.strip() for line in open('/content/drive/My Drive/Colab Notebooks/539_ANN/data/TheTimeMachine.txt') if line.strip()][2:]\n",
        "print(\"\\n\".join(corpus[:10]))\n",
        "\n",
        "corpus = [re.sub('[^A-Za-z0-9]+', ' ', line).lower() for line in corpus]\n",
        "corpus = [re.sub(' +', ' ', line) for line in corpus]\n",
        "corpus = [word for line in corpus for word in line.split()]\n",
        "\n",
        "vocab_size = 5000\n",
        "tkn_counter = Counter([word for word in corpus])\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(tkn_counter.most_common(vocab_size))}\n",
        "vocab[\"/UNK\"] = len(vocab)\n",
        "\n",
        "class TextCorpusDataset(tf.keras.utils.Sequence):\n",
        "    def __init__(self, corpus, vocab, snippet_len=50):\n",
        "        self.corpus = corpus\n",
        "        self.snippet_len = snippet_len\n",
        "        # Vocabulary (word-to-index mapping)\n",
        "        self.vocab = vocab\n",
        "        # Inverse vocabulary (index-to-word mapping)\n",
        "        self.inv_vocab = {idx: word for word, idx in self.vocab.items()}\n",
        "\n",
        "    def convert2idx(self, word_sequence):\n",
        "        return [self.vocab.get(word, self.vocab[\"/UNK\"]) for word in word_sequence]\n",
        "\n",
        "    def convert2words(self, idx_sequence):\n",
        "        return [self.inv_vocab[idx] for idx in idx_sequence]\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.corpus) - self.snippet_len) // self.snippet_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = idx * self.snippet_len\n",
        "        snippet = self.corpus[idx:idx+self.snippet_len]\n",
        "        snippet = np.array(self.convert2idx(snippet))\n",
        "        return snippet\n",
        "\n",
        "# Test dataset function\n",
        "dataset = TextCorpusDataset(??)\n",
        "snippet = dataset[123]\n",
        "print(\"\\nRandom snippet from the corpus.\")\n",
        "print(\"  * Token IDS:\\t\", snippet)\n",
        "print(\"  * Words:\\t\\t\", \" \".join([dataset.inv_vocab[i] for i in snippet]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define SimpleRNN\n",
        "\n",
        "class SimpleRNN(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, hidden_dim):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.vocab_size, self.hidden_dim = vocab_size, hidden_dim\n",
        "\n",
        "        self.linear_inp2state = layers.Dense(hidden_dim)\n",
        "        self.linear_state2state = layers.Dense(hidden_dim)\n",
        "        self.linear_state2out = layers.Dense(vocab_size)\n",
        "\n",
        "    def initial_state(self, batch_size, device):\n",
        "        return tf.zeros((batch_size, self.hidden_dim), dtype=tf.float32)\n",
        "\n",
        "    def call(self, inp_seq, state=None):\n",
        "        n_steps, batch_size = inp_seq.shape[:2]\n",
        "\n",
        "        # If state is not provided, get initial state.\n",
        "        if state is None:\n",
        "            state = self.initial_state(batch_size, inp_seq.device)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(n_steps):\n",
        "            state = tf.tanh(self.linear_inp2state(inp_seq[t]) + self.linear_state2state(state))\n",
        "            out = self.??\n",
        "            outputs.append(out)\n",
        "\n",
        "        return tf.stack(outputs, 0), state\n",
        "\n",
        "hidden_dim = 256\n",
        "model = SimpleRNN(??)\n",
        "\n",
        "sentence = \"today is too darn cold\".split()\n",
        "# text to index, must have size 5, 1, 4582 (Note that in RNNs the batch is often the 2nd dimension, not the first)\n",
        "inp = tf.constant(dataset.??)[:, tf.newaxis]\n",
        "inp = tf.one_hot(inp, len(vocab), dtype=tf.float32)\n",
        "Yhat, new_state = model(inp)\n",
        "Yhat = tf.argmax(Yhat, axis=-1)\n",
        "print(dataset.convert2words(??))\n"
      ],
      "metadata": {
        "id": "9f9cBFhBGd0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prefix, num_preds, model, vocab):\n",
        "    prefix = tf.constant(dataset.convert2idx(prefix.split()), dtype=tf.int32).numpy()\n",
        "\n",
        "    state, outputs = None, [prefix[0]]\n",
        "    for i in range(1, len(prefix) + num_preds):\n",
        "        # Prepare one token at a time to feed the model\n",
        "        inp = tf.one_hot(outputs[-1], len(vocab), dtype=tf.float32)[tf.newaxis, tf.newaxis]\n",
        "\n",
        "        # Compute the prediction for the next token\n",
        "        yhat, state = model(inp, state)\n",
        "\n",
        "        if i < len(prefix):\n",
        "            # During warmup (while parsing the prefix), we ignore the model prediction\n",
        "            outputs.append(prefix[i])\n",
        "        else:\n",
        "            # Otherwise, append the model prediction to the output list\n",
        "            yhat = tf.argmax(yhat[0, 0], axis=-1).numpy()\n",
        "            outputs.append(yhat)\n",
        "    return ' '.join([dataset.inv_vocab[tkn] for tkn in outputs])\n",
        "\n",
        "generate('i do not mean to ask you to accept anything', 10, model, vocab)"
      ],
      "metadata": {
        "id": "N9tE_8NGG1SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_on_sequence(seq, model, optimizer, unroll=5):\n",
        "    batch_size, num_tokens = seq.shape\n",
        "\n",
        "    total_loss, state = 0., None\n",
        "    for i in range(0, num_tokens-unroll-1, unroll):\n",
        "        if state is not None:\n",
        "            state = tf.stop_gradient(state)\n",
        "        # Define the input sequence along which we will unroll the RNN\n",
        "        x = tf.transpose(seq[:, i:i+??])\n",
        "        y = tf.transpose(seq[:, i+??:i+??+??])\n",
        "\n",
        "        x = tf.one_hot(x, len(vocab), dtype=tf.float32)\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_hat, state = model(x, state)\n",
        "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y, tf.reshape(y_hat, (-1, len(vocab))),from_logits=True))\n",
        "        total_loss += loss.numpy()\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    n_batches = (num_tokens-unroll-1) // unroll\n",
        "    return total_loss / n_batches\n",
        "\n",
        "def fit(model, loader, vocab, lr, num_epochs=100, unroll=5):\n",
        "    optimizer = tf.optimizers.RMSprop(lr)\n",
        "    test_prompt = 'i do not mean to ask you to accept anything'\n",
        "    loader_size = len(list(loader))\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for sequence in loader:\n",
        "            total_loss += train_on_sequence(sequence, model, optimizer, unroll=unroll)\n",
        "        total_loss /= loader_size\n",
        "\n",
        "        print(f'Epoch {epoch} | Perplexity {np.exp(total_loss):.1f}. Loss: {total_loss:.3f}')\n",
        "        print(generate(test_prompt, 50, model, vocab))\n",
        "\n",
        "num_epochs, lr = 100, 0.001\n",
        "dataset = TextCorpusDataset(corpus, vocab, 100)\n",
        "loader = tf.data.Dataset.from_generator(lambda: iter(dataset), output_signature=tf.TensorSpec(shape=(100,), dtype=tf.int32)).batch(32)\n",
        "model = SimpleRNN(len(vocab), hidden_dim)\n",
        "fit(model, loader, vocab, lr, num_epochs, unroll=5)"
      ],
      "metadata": {
        "id": "lNXbiSm3JAs4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}